{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP0189: Applied Artificial Intelligence\n",
    "## Week 2 (pipelines and model selection)\n",
    "\n",
    "### After this week you will be able to ...\n",
    "- Streamline preprocessing steps in advanced way (Pipeline and ColumnTransformer)\n",
    "- Perform model selection and model assessment using different partitions of the data\n",
    "- Use different cross-validation strategies \n",
    "- Use GridSearchCV \n",
    "\n",
    "### Acknowledgements\n",
    "- https://archive.ics.uci.edu/ml/datasets/adult\n",
    "- https://scikit-learn.org/stable/modules/compose.html\n",
    "- https://scikit-learn.org/stable/model_selection.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "Before we begin, we make sure that all necessary libraries are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn==1.6.1 matplotlib seaborn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load the same adult income dataset used in last week's lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_adult(data_path: str) -> pd.DataFrame:\n",
    "    # Load dataset with specifying ' ?' as missing values\n",
    "    names = ['age','workclass','fnlwgt','education',\n",
    "            'marital-status','occupation','relationship','race','sex',\n",
    "            'capital-gain','capital-loss','hours-per-week','native-country',\n",
    "            'y']\n",
    "    df = pd.read_csv(data_path, delimiter=',', names=names, na_values=' ?')\n",
    "\n",
    "    for col_name in df.columns:\n",
    "        if df[col_name].dtype == object:  # Checking for object type (string in pandas)\n",
    "            df[col_name] = df[col_name].apply(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def train_test_split_df(df: pd.DataFrame, test_ratio=0.3, target_col=\"y\", random_state=42) -> tuple[pd.DataFrame, npt.NDArray[np.bool_], pd.DataFrame, npt.NDArray[np.bool_]]:\n",
    "    # Separate features and target\n",
    "    df_data = df.drop(columns=[target_col])\n",
    "    df_target = df[target_col]\n",
    "\n",
    "    # Randomized train-test split with a fixed seed\n",
    "    train_X_df, test_X_df, train_y_df, test_y_df = train_test_split(\n",
    "        df_data, df_target, test_size=test_ratio, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Convert target variable to binary (assuming it's categorical with \">50K\" and others)\n",
    "    train_y = (train_y_df == \">50K\").to_numpy(dtype=np.bool_)\n",
    "    test_y = (test_y_df == \">50K\").to_numpy(dtype=np.bool_)\n",
    "\n",
    "    return train_X_df, train_y, test_X_df, test_y\n",
    "\n",
    "adult_df = load_adult(\"adult.data\")\n",
    "train_X, train_y, test_X, test_y = train_test_split_df(adult_df)\n",
    "\n",
    "train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Pipelines\n",
    "\n",
    "This part of the lab focuses on streamlining preprocessing and model training with scikit-learn. We will reimplement the last exercise of last week's lab using a more advanced way of controlling how to process data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1: Writing a Pipeline\n",
    "\n",
    "[Pipelines](https://scikit-learn.org/stable/modules/compose.html) are a feature of scikit-learn for combining multiple processing steps together. This is quite useful, for example, when we want to apply a sequence of many pre-processing steps to the same data.\n",
    "\n",
    "In this exercise, we extract the `age` feature from the dataset and add some missing values for the sake of exercise. Use a `Pipeline` to impute the missing values and scale the feature.\n",
    "\n",
    "> You can use the same imputation and scaling operations seen in the previous lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function for visualising preprocessing results\n",
    "def check_results(feature_name: str, age_original: npt.NDArray, age_preprocessed: npt.NDArray) -> None:\n",
    "    # Plot the data distribution before and after preprocessing\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    fig.suptitle(feature_name)\n",
    "    ax[0].hist(age_original)\n",
    "    ax[0].set_title(\"Original\")\n",
    "    ax[1].hist(age_preprocessed)\n",
    "    ax[1].set_title(\"Preprocessed\")\n",
    "    plt.show()\n",
    "\n",
    "    # Print the value of the first entry before and after preprocessing\n",
    "    print(f\"Original first entry: {age_original.ravel()[0]}\")\n",
    "    print(f\"Preprocessed first entry: {round(age_preprocessed.ravel()[0], 2)}\")\n",
    "\n",
    "# Set the first row of all columns to NaN to simulate missing values\n",
    "train_X.iloc[0] = np.nan\n",
    "test_X.iloc[0] = np.nan\n",
    "\n",
    "# Extract the age feature as a Numpy array\n",
    "age_original = train_X[\"age\"].to_numpy(dtype=np.float32).reshape((len(train_X[\"age\"]), 1))\n",
    "\n",
    "# Create a Pipeline to preprocess the age feature\n",
    "impute_and_scale = \n",
    "\n",
    "# Fit the pipeline and check results\n",
    "age_preprocessed = impute_and_scale.fit_transform(age_original)\n",
    "check_results(\"Age\", age_original, age_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we can now easily reapply the fitted preprocessing steps to the testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the age feature from the testing data\n",
    "age_original_test = test_X[\"age\"].to_numpy(dtype=np.float32).reshape((len(test_X[\"age\"]), 1))\n",
    "\n",
    "# Apply the fitted pipeline\n",
    "age_preprocessed_test = impute_and_scale.transform(age_original_test)\n",
    "\n",
    "check_results(\"Age\", age_original_test, age_preprocessed_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2: Deciding which Pipeline to run\n",
    "\n",
    "In the previous task, we only applied our pipeline to the `age` feature, which we extracted and passed to the fitting function. However, the same imputation and scaling strategy would be suitable for most numerical features in the dataset. By contrast, it would not make sense for categorical features (since they cannot be scaled and cannot be imputed by taking the mean). So we'd like to use the same pipeline for all numerical features, while leaving out categorical ones.\n",
    "\n",
    "Scikit-learn offers a [ColumnTransformer](https://scikit-learn.org/stable/modules/compose.html#columntransformer-for-heterogeneous-data) feature for this purpose. For this exercise, write a ColumnTransformer which takes the whole training dataset, selects only the numerical features and applies the pipeline from the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# List the features to process\n",
    "non_categorical_features = [\"age\", ...]\n",
    "\n",
    "# Apply the pipeline using ColumnTransformer\n",
    "preprocessor = \n",
    "\n",
    "# Fit the pipeline and check results\n",
    "non_categorical_preprocessed = \n",
    "\n",
    "for i, feature in enumerate(non_categorical_features):\n",
    "    check_results(feature, train_X[[feature]].to_numpy(), non_categorical_preprocessed[:, i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the fitted pipeline to the testing data is now even easier, as we don't need to manually extract the features we want to preprocess.\n",
    "\n",
    "Use the pipeline to transform the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_categorical_preprocessed = \n",
    "\n",
    "for i, feature in enumerate(non_categorical_features):\n",
    "    check_results(feature, test_X[[feature]].to_numpy(), non_categorical_preprocessed[:, i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3: Preprocessing the whole dataset\n",
    "\n",
    "Now expand the code you've written so far to process all features in the dataset. Apply whichever steps seem reasonable depending on the feature types.\n",
    "\n",
    "> You can reuse the same steps you used in last week's notebook, but use Pipelines to join them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your pipelines here\n",
    "\n",
    "# Assign different pipelines to different features\n",
    "\n",
    "# Fit the combined preprocessing pipeline to the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.4: Joining preprocessing and model training\n",
    "\n",
    "Note that any estimator can be part of a Pipeline - even models. Try to create a new pipeline which combines the `preprocessing_pipeline` from the previous task with an SVM classifier. The resulting pipeline will be able to both fit the preprocessing pipeline and train the model in a single step.\n",
    "\n",
    "> You can use `make_pipeline` to add steps to an existing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the fitted pipeline to preprocess the test data and predict results in a single step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Cross Validation (CV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn provides a nice visualisation of various cross validation methods.\n",
    "This notebook focuses on different cross validation strategies and how to account for data structure during cross-validation.\n",
    "\n",
    "\n",
    "Visit: https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#visualizing-cross-validation-behavior-in-scikit-learn\n",
    "\n",
    "![kfold](https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_006.png)\n",
    "![stra-kfold](https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_003.png)\n",
    "![group-kfold](https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_004.png)\n",
    "![stra-group-kfold](https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_010.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import (\n",
    "    KFold,\n",
    "    StratifiedKFold,\n",
    "    GroupKFold,\n",
    "    StratifiedGroupKFold,\n",
    "    GridSearchCV,\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def perform_cv(X_train: pd.DataFrame, y_train: npt.NDArray, pipeline: Pipeline, cv_strategy) -> tuple[float, float]:\n",
    "    ''' \n",
    "    Return the mean accuracy and its standard deviation\n",
    "    when fitting the given pipeline across several cross-validation folds,\n",
    "    according to the given CV strategy\n",
    "    '''\n",
    "\n",
    "    scores = cross_val_score(\n",
    "        pipeline,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=cv_strategy,\n",
    "        n_jobs=-1,\n",
    "        scoring='accuracy'\n",
    "    )\n",
    "    \n",
    "    return scores.mean(), scores.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Cross-validation for hyperparameter optimisation\n",
    "Now apply cross-validation to the train set for optimizing the models hyperparameters. After identifying the best hyperparameter, measure the performance on the test data by training the models on the training data using the optimal hyperparameter. Print the average cross-validation score, the best cross-validation score, the best hyperparameter and the test-score.\n",
    "\n",
    " - Is there a difference between the average cross-validation score, the best cross-validation score and the test-score?\n",
    "\n",
    "Note: remember that the pre-processing steps, including data centering and scaling should be embedded in the CV.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Step 1: Cross-Validation for Hyperparameter Tuning ##############\n",
    "# Define hyperparameter search space\n",
    "C_values = [0.1, 0.5, 1.0]  # Regularization parameter\n",
    "kernel_values = ['linear', 'rbf']  # Kernel types\n",
    "cv_strategy = KFold(n_splits=2)  # 2-fold cross-validation\n",
    "\n",
    "# Try all combinations and store results\n",
    "svc_cv_results = []\n",
    "for C in C_values:\n",
    "    for kernel in kernel_values:\n",
    "        mean_score, std_score = \n",
    "        svc_cv_results.append((C, kernel, mean_score, std_score))\n",
    "\n",
    "############## Step 2: Train Final Model with Best Parameters ##############\n",
    "# Find best parameters based on CV results\n",
    "best_svc_params = max(svc_cv_results, key=lambda x: x[2])\n",
    "print(f\"\\nBest parameters: C={best_svc_params[0]}, kernel={best_svc_params[1]}\")\n",
    "\n",
    "# Create and train final model with best parameters\n",
    "\n",
    "\n",
    "############## Step 3: Evaluate on Test Set ##############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5-1: Plotting model performance\n",
    "Plot the model performance (mean accuracy and SD) for different hyper-parameter values.\n",
    "- How does the accuracy vary as function of the hyperparameter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Group Stratified cross-validation\n",
    "Repeat task 5 using groups stratified CV with k=5 and sex as the group variable. Centre and scale the data before training the models. Print the average cross-validation score, the best cross-validation score, the best hyperparameter and the test-score.\n",
    "\n",
    "- Did the performances changes with the group stratified CV?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7: Using Scikit's GridSearchCV\n",
    "So far, we manually calculated the accuracy score of each cross-validation fold using the `perform_cv` function. This allowed us to show how cross-validation works in detail, but in practice we'd probably use the utilities provided by Scikit instead. One such utility is [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), a Scikit class to automatically perform hyperparameter optimisation by calculating the CV score for a \"grid\" of hyperparameters.\n",
    "\n",
    "Here, rewrite the hyperparameter optimisation code so that it uses GridSearchCV to search the hyperparameters grid and train the final model.\n",
    "\n",
    "See also the [user guide](https://scikit-learn.org/stable/modules/grid_search.html) for hyperparameter optimisation with Scikit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Step 1: Define Model and Pipeline ##############\n",
    "# Create a pipeline that combines preprocessing and SVC model\n",
    "# This ensures preprocessing steps are properly included in cross-validation\n",
    "model_svc = make_pipeline(preprocessor, SVC(random_state=42))\n",
    "\n",
    "\n",
    "############## Step 2: Define Hyperparameter Search Space ##############\n",
    "# Specify the parameters to search\n",
    "# Note: 'svc__' prefix is required because SVC is part of a pipeline\n",
    "param_grid = {\n",
    "    'svc__C': [0.1, 1, 10],          # Regularization parameter\n",
    "    'svc__kernel': ['linear', 'rbf']  # Kernel types\n",
    "}\n",
    "\n",
    "\n",
    "############## Step 3: Setup Cross-validation Strategy ##############\n",
    "# Use StratifiedKFold to maintain the same ratio of samples for each class\n",
    "\n",
    "\n",
    "############## Step 4: Configure Grid Search ##############\n",
    "# Initialize GridSearchCV with all parameters\n",
    "\n",
    "\n",
    "############## Step 5: Perform Grid Search and fit the final model ##############\n",
    "# Fit the grid search to find best parameters\n",
    "# This step performs cross-validation for all parameter combinations\n",
    "\n",
    "\n",
    "############## Step 6: Make Predictions ##############\n",
    "# Apply model to test set\n",
    "\n",
    "\n",
    "############## Step 7: Print Results ##############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8: Nested cross-validation\n",
    "Now implement a nested CV for optimize the models’ hyper-parameters and assessing the models’ performance (with k=5 for the inner and outer loop). The inner loop should optimize the models’ hyper-parameters and the outer loop should assess the models’ performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
