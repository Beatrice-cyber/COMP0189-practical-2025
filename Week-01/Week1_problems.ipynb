{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34034f63",
   "metadata": {
    "id": "34034f63"
   },
   "source": [
    "# COMP0189: Applied Artificial Intelligence\n",
    "## Week 1 (Data Preprocessing)\n",
    "\n",
    "### After this week you will be able to ...\n",
    "- Load datasets using scikit-learn.\n",
    "- Appreciate the importance of exploratory data analysis (EDA).\n",
    "- Learn and apply various preprocessing techniques (scaling, encoding, handling missing values).\n",
    "- Compare the impact of preprocessing on model performance.\n",
    "\n",
    "### Acknowledgements\n",
    "- https://github.com/UCLAIS/Machine-Learning-Tutorials\n",
    "- https://www.cs.columbia.edu/~amueller/comsw4995s19/schedule/\n",
    "- https://scikit-learn.org/stable/\n",
    "- https://archive.ics.uci.edu/ml/datasets/adult"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28f1108",
   "metadata": {
    "id": "a28f1108"
   },
   "source": [
    "## Introduction to Scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Uan0H5pXgXeK",
   "metadata": {
    "id": "Uan0H5pXgXeK"
   },
   "source": [
    "Why do we use sklearn??\n",
    "\n",
    "1. Example Datasets\n",
    "    - sklearn.datasets : Provides example datasets\n",
    "\n",
    "2. Feature Engineering  \n",
    "    - sklearn.preprocessing : Variable functions as to data preprocessing\n",
    "    - sklearn.feature_selection : Help selecting primary components in datasets\n",
    "    - sklearn.feature_extraction : Vectorised feature extraction\n",
    "    - sklearn.decomposition : Algorithms regarding Dimensionality Reduction\n",
    "\n",
    "3. Data split and Parameter Tuning  \n",
    "    - sklearn.model_selection : 'Train Test Split' for cross validation, Parameter tuning with GridSearch\n",
    "\n",
    "4. Evaluation  \n",
    "    - sklearn.metrics : accuracy score, ROC curve, F1 score, etc.\n",
    "\n",
    "5. ML Algorithms\n",
    "    - sklearn.ensemble : Ensemble, etc.\n",
    "    - sklearn.linear_model : Linear Regression, Logistic Regression, etc.\n",
    "    - sklearn.naive_bayes : Gaussian Naive Bayes classification, etc.\n",
    "    - sklearn.neighbors : Nearest Centroid classification, etc.\n",
    "    - sklearn.svm : Support Vector Machine\n",
    "    - sklearn.tree : DecisionTreeClassifier, etc.\n",
    "    - sklearn.cluster : Clustering (Unsupervised Learning)\n",
    "\n",
    "6. Utilities  \n",
    "    - sklearn.pipeline: pipeline of (feature engineering -> ML Algorithms -> Prediction)\n",
    "\n",
    "7. Train and Predict  \n",
    "    - fit()\n",
    "    - predict()\n",
    "\n",
    "8. and more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb38f49",
   "metadata": {
    "id": "2bb38f49"
   },
   "outputs": [],
   "source": [
    "%pip install scikit-learn==1.6.1 matplotlib seaborn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0eca34",
   "metadata": {
    "id": "7e0eca34"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0oZl2671ezNH",
   "metadata": {
    "id": "0oZl2671ezNH"
   },
   "source": [
    "## 1. Exploratory Data Analysis (EDA)\n",
    "In this section, you will use two datasets to illustrate EDA for: 1) a regression task and 2) a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed16e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import Bunch\n",
    "\n",
    "def load_boston():\n",
    "    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "    raw_df = pd.read_csv(data_url, sep=\"\\\\s+\", skiprows=22, header=None)\n",
    "    row_first_lines = raw_df.values[::2, :]\n",
    "    row_second_lines = raw_df.values[1::2, :2]\n",
    "\n",
    "    description_file = open(\"boston_description.txt\", \"r\")\n",
    "    \n",
    "    data= np.hstack([row_first_lines[~np.isnan(row_first_lines)].reshape(506, 11), row_second_lines])\n",
    "    target = raw_df.values[1::2, 2]\n",
    "    feature_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "    description = description_file.read()\n",
    "\n",
    "    return Bunch(data=data, target=target, feature_names=feature_names, DESCR=description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5fdd50",
   "metadata": {
    "id": "6b5fdd50"
   },
   "source": [
    "### **1.1 Boston House Price Dataset** (regression task)\n",
    "\n",
    "Let's first take a look at the Boston House Price dataset. This Dataset is deprecated as of version 1.2, but we will use this for educational purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1-skT2pnfKU1",
   "metadata": {
    "id": "1-skT2pnfKU1"
   },
   "source": [
    "\n",
    "\n",
    "> Take some time to look at the different predictor variables. What do they mean and how do you expect them to influence the target variable (median house price)?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8783fee",
   "metadata": {
    "id": "d8783fee"
   },
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111eb500",
   "metadata": {
    "id": "111eb500"
   },
   "outputs": [],
   "source": [
    "boston.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fffb8b",
   "metadata": {
    "id": "52fffb8b"
   },
   "outputs": [],
   "source": [
    "print('Boston dataset feature names: ', boston.feature_names)\n",
    "print('Number of features: ', len(boston.feature_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MBPCAT_NffRt",
   "metadata": {
    "id": "MBPCAT_NffRt"
   },
   "source": [
    "**Exploratory data analysis**\n",
    "\n",
    "Why is this useful?\n",
    "- Understand dataset characetristic in more detail (range, distribution, inter-variable relationships).\n",
    "- Identify necessary preprocessing steps (handle missing values and outliers, encoding and scaling features..)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LoqaVd3WggMo",
   "metadata": {
    "id": "LoqaVd3WggMo"
   },
   "outputs": [],
   "source": [
    "# convert the dataset into a dataframe\n",
    "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "# extract the target variable\n",
    "df['MEDV'] = boston.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XFC1k-jzhd6L",
   "metadata": {
    "id": "XFC1k-jzhd6L"
   },
   "source": [
    "Basic statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Y4nflxmNhbHE",
   "metadata": {
    "id": "Y4nflxmNhbHE"
   },
   "outputs": [],
   "source": [
    "print(\"Dataset sample:\")\n",
    "print(df.head())\n",
    "print(\"Basic statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cRItmn7hh3Ke",
   "metadata": {
    "id": "cRItmn7hh3Ke"
   },
   "source": [
    "Visualise the feature and target distributions - are all features continuous?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pyMMwXKSh2h5",
   "metadata": {
    "id": "pyMMwXKSh2h5"
   },
   "outputs": [],
   "source": [
    "# Your code here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iEFceQQNlwxz",
   "metadata": {
    "id": "iEFceQQNlwxz"
   },
   "source": [
    "Are there any missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TokP8vOrlvvY",
   "metadata": {
    "id": "TokP8vOrlvvY"
   },
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9n_xF8SVi9tu",
   "metadata": {
    "id": "9n_xF8SVi9tu"
   },
   "source": [
    "Are there any outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1Paz32AohzSb",
   "metadata": {
    "id": "1Paz32AohzSb"
   },
   "source": [
    "> Hint: you can use boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TLiYcCRXi_YS",
   "metadata": {
    "id": "TLiYcCRXi_YS"
   },
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48oMFUuhjq5j",
   "metadata": {
    "id": "48oMFUuhjq5j"
   },
   "source": [
    "Investigate the relationships between features and the outcome variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "E3gfAlN6h6Zf",
   "metadata": {
    "id": "E3gfAlN6h6Zf"
   },
   "source": [
    "> Hint: a correlation map may be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_JqrBSsujuOJ",
   "metadata": {
    "id": "_JqrBSsujuOJ"
   },
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58cc370",
   "metadata": {
    "id": "e58cc370"
   },
   "source": [
    "See how our data are spread in different ranges. 3rd feature (CHAS) is even in binary. Most of the algorithms perform poorly on these various input spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uSYgfIK3mQOy",
   "metadata": {
    "id": "uSYgfIK3mQOy"
   },
   "source": [
    "Based on the EDA, what do you observe?\n",
    "- Do some features need encoding?\n",
    "- Do all feature share a similar range, or would they need scaling?\n",
    "- Are there any outliers or missing values that need to be taken care of?\n",
    "\n",
    "By addressing these questions, we inform our preprocessing choices and make sure that the data is properly prepared for the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e949cac4",
   "metadata": {
    "id": "e949cac4"
   },
   "source": [
    "### **1.2 Wine Dataset** (classification task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936d1584",
   "metadata": {
    "id": "936d1584"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17250624",
   "metadata": {
    "id": "17250624"
   },
   "outputs": [],
   "source": [
    "# Load and describe the dataset\n",
    "wine = load_wine()\n",
    "print(wine.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e30564c",
   "metadata": {
    "id": "9e30564c"
   },
   "outputs": [],
   "source": [
    "wine.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2zDNb0M0rExV",
   "metadata": {
    "id": "2zDNb0M0rExV"
   },
   "source": [
    "**Exploratory data analysis**\n",
    "\n",
    "Just like for the Boston housing dataset, you need to analyse the dataset's features and predictor variable relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MYY3oNPDrYuE",
   "metadata": {
    "id": "MYY3oNPDrYuE"
   },
   "outputs": [],
   "source": [
    "# convert into a dataframe\n",
    "df = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "# extract the target variable\n",
    "df['Class'] = wine.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sXl2o3vRrn0l",
   "metadata": {
    "id": "sXl2o3vRrn0l"
   },
   "source": [
    "Basic statistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "azpwWnXVrzhe",
   "metadata": {
    "id": "azpwWnXVrzhe"
   },
   "source": [
    "\n",
    "\n",
    "> Is the dataset balanced?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hEa_2dpFrjX4",
   "metadata": {
    "id": "hEa_2dpFrjX4"
   },
   "outputs": [],
   "source": [
    "print(\"Dataset sample: \")\n",
    "print(df.head())\n",
    "print(\"Basic statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"Dataset balance: \", df['Class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Z984Um6tiEfP",
   "metadata": {
    "id": "Z984Um6tiEfP"
   },
   "source": [
    "How are the featues distributed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2jRZ4E24rsK8",
   "metadata": {
    "id": "2jRZ4E24rsK8"
   },
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4_qN0rgrsMYd",
   "metadata": {
    "id": "4_qN0rgrsMYd"
   },
   "source": [
    "Are there any missing values? Outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ftVg2wVKtcnW",
   "metadata": {
    "id": "ftVg2wVKtcnW"
   },
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GjiFrOjas9zN",
   "metadata": {
    "id": "GjiFrOjas9zN"
   },
   "source": [
    "How do the feature correlate with the outcome?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mHioBpUHiU9t",
   "metadata": {
    "id": "mHioBpUHiU9t"
   },
   "source": [
    "> Hint: plot feature distributions stratified on outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wxXmCVfzsQve",
   "metadata": {
    "id": "wxXmCVfzsQve"
   },
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MkUeykvIJrXA",
   "metadata": {
    "id": "MkUeykvIJrXA"
   },
   "source": [
    "## 2. Investigating impact of pre-processing on differentent models\n",
    "\n",
    "In this section, you have to design: <br>\n",
    "<br>1) A model to predict the wine class of different wine samples (wine dataset).\n",
    "<br>2) A model to predict the house price class (boston dataset).\n",
    "\n",
    "Try out different models and compare them.\n",
    "\n",
    "\n",
    "Based on the EDA, what preprocessing steps are needed? Try to compare different preprocessing methods (e.g., feature scaling methods) to assess their impact on the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4JAzm3zNK6sr",
   "metadata": {
    "id": "4JAzm3zNK6sr"
   },
   "source": [
    "Helpful imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RqNfSfh-K6V9",
   "metadata": {
    "id": "RqNfSfh-K6V9"
   },
   "outputs": [],
   "source": [
    "# some models..\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "# evaluation metrics..\n",
    "from sklearn.metrics import accuracy_score, root_mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "# different scalers to try out..\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n",
    "\n",
    "# if you want to make a pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# to split data into train and test sets..\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee6973e",
   "metadata": {
    "id": "cee6973e"
   },
   "source": [
    "### Example: Impact of feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5dede2",
   "metadata": {
    "id": "8e5dede2"
   },
   "source": [
    "Normalization scales each input variable separately to the range 0-1.  \n",
    "Standardization scales each input variable separately by subtracting the mean (centering) and dividing each of them by the standard deviation to shift the distribution to have a mean of zero and a standard deviation of one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96b1bba",
   "metadata": {
    "id": "c96b1bba"
   },
   "source": [
    "#### Example usage of sklearn.preprocessing.StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce7c672",
   "metadata": {
    "id": "4ce7c672"
   },
   "outputs": [],
   "source": [
    "# Example\n",
    "unscaled_data = np.asarray([[100, 0.001],\n",
    " [8, 0.05],\n",
    " [50, 0.005],\n",
    " [88, 0.07],\n",
    " [4, 0.1]])\n",
    "# define standard scaler\n",
    "scaler = StandardScaler()\n",
    "# transform data\n",
    "scaled_data = scaler.fit_transform(unscaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c79b31b",
   "metadata": {
    "id": "4c79b31b"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(unscaled_data).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b7ceb7",
   "metadata": {
    "id": "54b7ceb7"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(scaled_data).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1244e3e6",
   "metadata": {
    "id": "1244e3e6"
   },
   "outputs": [],
   "source": [
    "del scaled_data, unscaled_data, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe24e99f",
   "metadata": {
    "id": "fe24e99f"
   },
   "source": [
    "**Tasks**  \n",
    "- Try using different scaling methods, such as MinMaxScaler and Normalisation. Do you see the difference in the histogram?\n",
    "- Experiment the effects of different feature scaling methods on various ML algorithms e.g. KNN, SVM, Decision-Tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465fa5bd",
   "metadata": {
    "id": "465fa5bd"
   },
   "source": [
    "#### Scaling Vs. Unscaling the Wine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdfa4ba",
   "metadata": {
    "id": "ffdfa4ba"
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "# We are using the wine dataset\n",
    "features, target = load_wine(return_X_y=True)\n",
    "\n",
    "# Make a train/test split using 30% test size\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.30, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jF8ckAVlYkk2",
   "metadata": {
    "id": "jF8ckAVlYkk2"
   },
   "outputs": [],
   "source": [
    "# Define scalers and models\n",
    "scalers = {\n",
    "    # None\n",
    "}\n",
    "\n",
    "models = {\n",
    "    # None\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "# Iterate over each scaler\n",
    "#None\n",
    "\n",
    "\n",
    "\n",
    "    # Iterate over each model\n",
    "    #None\n",
    "\n",
    "\n",
    "        # Fit and predict with unscaled and scaled data\n",
    "        model.fit(X_train, y_train)\n",
    "        unscaled_y_hat = model.predict(X_test)\n",
    "        unscaled_acc = accuracy_score(y_test, unscaled_y_hat)\n",
    "\n",
    "        model.fit(scaled_X_train, y_train)\n",
    "        scaled_y_hat = model.predict(scaled_X_test)\n",
    "        scaled_acc = accuracy_score(y_test, scaled_y_hat)\n",
    "\n",
    "        # Store results\n",
    "        results[key] = {\n",
    "            'Unscaled Accuracy': unscaled_acc,\n",
    "            'Scaled Accuracy': scaled_acc\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p6WG2aZnU9Xd",
   "metadata": {
    "id": "p6WG2aZnU9Xd"
   },
   "outputs": [],
   "source": [
    "# Load the Boston dataset\n",
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define scalers and models\n",
    "scalers = {\n",
    "    #None\n",
    "}\n",
    "\n",
    "models = {\n",
    "    #None\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "# Iterate over each scaler\n",
    "#None\n",
    "\n",
    "\n",
    "    # Iterate over each model\n",
    "    #None\n",
    "\n",
    "\n",
    "        # Fit and predict with unscaled and scaled data\n",
    "        model.fit(X_train, y_train)\n",
    "        unscaled_y_hat = model.predict(X_test)\n",
    "        unscaled_rmse = root_mean_squared_error(y_test, unscaled_y_hat, squared=False)\n",
    "        unscaled_r2 = r2_score(y_test, unscaled_y_hat)\n",
    "\n",
    "        model.fit(scaled_X_train, y_train)\n",
    "        scaled_y_hat = model.predict(scaled_X_test)\n",
    "        scaled_rmse = root_mean_squared_error(y_test, scaled_y_hat, squared=False)\n",
    "        scaled_r2 = r2_score(y_test, scaled_y_hat)\n",
    "\n",
    "        # Store results\n",
    "        results[key] = {\n",
    "            'Unscaled RMSE': unscaled_rmse,\n",
    "            'Unscaled R2': unscaled_r2,\n",
    "            'Scaled RMSE': scaled_rmse,\n",
    "            'Scaled R2': scaled_r2\n",
    "        }\n",
    "\n",
    "# Convert results to DataFrame for better readability\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b715b4",
   "metadata": {
    "id": "91b715b4"
   },
   "source": [
    "### Example: Impact of different preprocessing strategy in train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caf828b",
   "metadata": {
    "id": "5caf828b"
   },
   "source": [
    "Do you see the difference in RMSE?  \n",
    "**Question**  \n",
    "Above, we also scaled the test set.   \n",
    "Using the same code, see what happens if you don't scale the test data and predict based on the unscaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VURv48wAZ6k9",
   "metadata": {
    "id": "VURv48wAZ6k9"
   },
   "outputs": [],
   "source": [
    "# Store results\n",
    "results = {}\n",
    "\n",
    "# Iterate over each scaler\n",
    "for scaler_name, scaler in scalers.items():\n",
    "    #None\n",
    "\n",
    "    # Iterate over each model\n",
    "    for model_name, model in models.items():\n",
    "        #None\n",
    "\n",
    "        # Fit with scaled data\n",
    "\n",
    "\n",
    "        # Predict with unscaled test data\n",
    "\n",
    "\n",
    "        # Predict with scaled test data\n",
    "\n",
    "\n",
    "        # Store results\n",
    "        results[key] = {\n",
    "            'RMSE with Unscaled Test Data': unscaled_rmse,\n",
    "            'RMSE with Scaled Test Data': scaled_rmse\n",
    "        }\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df\n",
    "\n",
    "# Display results\n",
    "#for key, value in results.items():\n",
    "#    print(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232ba604",
   "metadata": {
    "id": "232ba604"
   },
   "source": [
    "## 3. Working with a messier dataset: **Adult Census Dataset**\n",
    "\n",
    "Classification task: predict whether income >50K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "R4nTLrPq7XEI",
   "metadata": {
    "id": "R4nTLrPq7XEI"
   },
   "source": [
    "### 3.1 Exploratory data analysis\n",
    "Same as before, analyse the dataset to identify the necessary preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IuXuK5xF7d4M",
   "metadata": {
    "id": "IuXuK5xF7d4M"
   },
   "outputs": [],
   "source": [
    "# Open the csv file and skim through it. It does not have column names\n",
    "# so we will allocate names to each column\n",
    "\n",
    "# Naming the Columns\n",
    "names = ['age','workclass','fnlwgt','education',\n",
    "        'marital-status','occupation','relationship','race','sex',\n",
    "        'capital-gain','capital-loss','hours-per-week','native-country',\n",
    "        'y']\n",
    "\n",
    "# Load dataset with specifying ' ?' as missing values\n",
    "df = pd.read_csv('adult.data', delimiter=',', names=names, na_values=' ?')\n",
    "\n",
    "# Number of observations\n",
    "print(\"Number of observations: \", len(df))\n",
    "\n",
    "# Look at sample of data\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hobklXSd9jeE",
   "metadata": {
    "id": "hobklXSd9jeE"
   },
   "source": [
    "Basis statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gQnGc0qW9kbW",
   "metadata": {
    "id": "gQnGc0qW9kbW"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mH3qT4Ls-M-z",
   "metadata": {
    "id": "mH3qT4Ls-M-z"
   },
   "source": [
    "Is the datset balanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aRhirBo97ni",
   "metadata": {
    "id": "8aRhirBo97ni"
   },
   "outputs": [],
   "source": [
    "print(\"Dataset balance: \", df['y'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nimOlexeAtdB",
   "metadata": {
    "id": "nimOlexeAtdB"
   },
   "source": [
    "Disribution of features and relationship to target variabe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_zLIUYZJD8DZ",
   "metadata": {
    "id": "_zLIUYZJD8DZ"
   },
   "source": [
    "> Hint: plot feature distribution stratified on the predictor variable\n",
    "\n",
    "> Inspect the range and cardinality of the different features. Is the range consistent across the numerical features? How many categories do categorical variables have?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e43nSpa-VEi",
   "metadata": {
    "id": "8e43nSpa-VEi"
   },
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G7Nfu5m-82wi",
   "metadata": {
    "id": "G7Nfu5m-82wi"
   },
   "source": [
    "Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AyRpMIOErkHo",
   "metadata": {
    "id": "AyRpMIOErkHo"
   },
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "An_vzHiIroYf",
   "metadata": {
    "id": "An_vzHiIroYf"
   },
   "outputs": [],
   "source": [
    "# Example the 15th row of the DataFrame - notice NaN\n",
    "row_15 = df.iloc[14]\n",
    "print(row_15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VPY9OP7fAzoC",
   "metadata": {
    "id": "VPY9OP7fAzoC"
   },
   "source": [
    "### 3.2 Data preprocessing\n",
    "Based on the EDA, what type of preprocessing is needed?\n",
    "- Do features need to be scaled and encoded?\n",
    "- Do missing values need to be imputed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FK7C-wAnISEl",
   "metadata": {
    "id": "FK7C-wAnISEl"
   },
   "source": [
    "A quick data fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L7jFzTUYwCe8",
   "metadata": {
    "id": "L7jFzTUYwCe8"
   },
   "outputs": [],
   "source": [
    "# For now, we will drop the rows with missing (NA) values\n",
    "df = df.dropna()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f0773d",
   "metadata": {
    "id": "94f0773d"
   },
   "outputs": [],
   "source": [
    "# Task: Get the unique values in the race and y column\n",
    "df['race'].unique()\n",
    "df['race'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c51abd",
   "metadata": {
    "id": "81c51abd"
   },
   "outputs": [],
   "source": [
    "# We see redundant space prefix in the values. Remove them.\n",
    "df['race'] = df['race'].apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a747d2",
   "metadata": {
    "id": "71a747d2"
   },
   "outputs": [],
   "source": [
    "df['race'].unique(), df['y'].unique(), df['occupation'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ded9523",
   "metadata": {
    "id": "8ded9523"
   },
   "source": [
    "Hmmm it's not just the race and y column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a575e39b",
   "metadata": {
    "id": "a575e39b"
   },
   "outputs": [],
   "source": [
    "# Let's try to apply this to all the string-valued columns\n",
    "for col_name in df.columns:\n",
    "    if df[col_name].dtype == object:  # Checking for object type (string in pandas)\n",
    "        df[col_name] = df[col_name].apply(lambda x: x.strip() if isinstance(x, str) else x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XvtPKB9EIJHi",
   "metadata": {
    "id": "XvtPKB9EIJHi"
   },
   "source": [
    "Check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e919df",
   "metadata": {
    "id": "78e919df"
   },
   "outputs": [],
   "source": [
    "for col_name in df.columns:\n",
    "    if not 'int' in str(df[col_name].dtype):\n",
    "        print(df[col_name].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51257e6",
   "metadata": {
    "id": "b51257e6"
   },
   "source": [
    "All done! You can now start data preprocessing such as encoding and imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ObSzNccWFulF",
   "metadata": {
    "id": "ObSzNccWFulF"
   },
   "source": [
    "#### **TASK 1: Encoding categorical variables** (label/ordinal encoding & one-hot encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9IXbM1FreY",
   "metadata": {
    "id": "0b9IXbM1FreY"
   },
   "source": [
    "Important: We need special care when we are encoding categorical variables\n",
    "\n",
    "**1. Take care of the missing values**\n",
    "- Beware not to encode missing values unless you are intending to do so.\n",
    "- Sometimes you want to encode missing values to a separate cateogory. For example, when you want to predict if passengers of titanic had survived or not, missing data of certain features can actually have meaning, i.e., Cabin information can be missing because the body was not found.\n",
    "\n",
    "**2. Know which encoding and scaling method you should select**\n",
    "- If your categories are ordinal, then it makes sense to use a LabelEncoder with a MinMaxScaler. For example, you can encode [low, medium, high], as [1,2,3], i.e., distance between low to high is larger than that of medium and high.\n",
    "\n",
    "- However, if you have non-ordinal categorical values, like [White, Hispanic, Black, Asian], then it would be better to use a OneHotEncoder instead of forcing ordinality with a LabelEncoder. Otherwise the algorithms you use (especially distance based algorithms like KNN) will make the assumption that the distance between White and Asian is larger than White and Hispanic, which is nonsensical.\n",
    "\n",
    "**3. Split before you encode to avoid data leakage**\n",
    "- If training a model using train/ test slit, you should split the dataset before you encode your data. It is natural for algorithms to see unknown values in the validation/test set that was not appearing in the train set. `sklearn.preprocessing.OneHotEncoder` is good at handling these unknown categories (`handle_unknown` parameter).\n",
    "\n",
    "- Discussion: What if you are certain about all the possible categories that can appear for each feature? Can you encode all the values before splitting the dataset into train and test set?\n",
    "\n",
    "\n",
    "This notebook shows the three points in the following sections with examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FTLlCSa4JG64",
   "metadata": {
    "id": "FTLlCSa4JG64"
   },
   "source": [
    "\n",
    "\n",
    "> What type of encoding is most appropriate for the different categorical features? For example, should education and native-country be encoded using the same technique?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116f91d9",
   "metadata": {
    "id": "116f91d9"
   },
   "outputs": [],
   "source": [
    "# Import encoders from sklearn\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r1hzAqc8eW_V",
   "metadata": {
    "id": "r1hzAqc8eW_V"
   },
   "outputs": [],
   "source": [
    "# Ordinal Encoding for 'education'\n",
    "# Define the order of education categories\n",
    "education_order = ['Preschool', '1st-4th', '5th-6th', '7th-8th', '9th', '10th', '11th', '12th', 'HS-grad', 'Some-college', 'Assoc-voc', 'Assoc-acdm', 'Prof-school', 'Bachelors', 'Masters', 'Doctorate']\n",
    "# Initialize the OrdinalEncoder with the specified categories\n",
    "ordinal_encoder = #None\n",
    "# Apply the OrdinalEncoder to the 'education' column\n",
    "df['education_encoded'] = #None\n",
    "\n",
    "# Check resulting education ordering\n",
    "edu_map = {}\n",
    "for i, row in df[[\"education\", \"education_encoded\"]].iterrows():\n",
    "    education = row[\"education\"]\n",
    "    edu_num = row[\"education_encoded\"]\n",
    "\n",
    "    if education not in edu_map:\n",
    "        edu_map.update({education: edu_num})\n",
    "    else:\n",
    "        assert edu_map[education] == edu_num\n",
    "edu_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SLbIX1WsJhRf",
   "metadata": {
    "id": "SLbIX1WsJhRf"
   },
   "outputs": [],
   "source": [
    "# OneHotEncoding for nominal features without an implied order\n",
    "# Including the previously missed nominal columns\n",
    "nominal_columns = #None\n",
    "onehot_encoder = #None\n",
    "onehot_encoded_columns = #None\n",
    "column_names = onehot_encoder.get_feature_names_out(nominal_columns)\n",
    "df_onehot_encoded = pd.DataFrame(onehot_encoded_columns, columns=column_names)\n",
    "\n",
    "# Integrate these new columns back into the original dataframe\n",
    "df = df.reset_index(drop=True)  # Reset index to align with the new onehot encoded DataFrame\n",
    "df = pd.concat([df, df_onehot_encoded], axis=1)\n",
    "\n",
    "# Optionally, remove the categorical columns if no longer needed\n",
    "df.drop(columns=nominal_columns + ['education'], inplace=True)\n",
    "\n",
    "# Label Encoding for the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "df['y_encoded'] = label_encoder.fit_transform(df['y'])\n",
    "\n",
    "# Remove the original 'y' column if no longer needed\n",
    "df.drop(columns=['y'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m2lhQLzSJtJP",
   "metadata": {
    "id": "m2lhQLzSJtJP"
   },
   "source": [
    "Check your encoding results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u-nFd7-sJujY",
   "metadata": {
    "id": "u-nFd7-sJujY"
   },
   "outputs": [],
   "source": [
    "# Display the first few rows of the modified DataFrame\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897db437",
   "metadata": {
    "id": "897db437"
   },
   "source": [
    "#### **TASK 2: Dealing with missing data** - imputation strategies\n",
    "\n",
    "In processing the data earlier, we did not take account of the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bf7cbc",
   "metadata": {
    "id": "47bf7cbc"
   },
   "outputs": [],
   "source": [
    "# Re-Load dataset with specifying ' ?' as missing values\n",
    "df = pd.read_csv('adult.data', delimiter=',', names=names, na_values=' ?')\n",
    "# Remove redundant space (same as before)\n",
    "for col_name in df.columns:\n",
    "    if df[col_name].dtype == object:  # Checking for object type (string in pandas)\n",
    "        df[col_name] = df[col_name].apply(lambda x: x.strip() if isinstance(x, str) else x) # Remove redundant space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VU_THgcZK4Rv",
   "metadata": {
    "id": "VU_THgcZK4Rv"
   },
   "source": [
    "**Task**: Create 3 train/test datasets using different methods for dealing with missing data:\n",
    "<br>A: Drop missing values, B: KNN imputation, C: Most frequent imputation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d1064d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "def train_test_split_df(df, test_ratio=0.3, target_col=\"y\", random_state=42):\n",
    "    # Separate features and target\n",
    "    df_data = df.drop(columns=[target_col])\n",
    "    df_target = df[target_col]\n",
    "\n",
    "    # Randomized train-test split with a fixed seed\n",
    "    train_X_df, test_X_df, train_y_df, test_y_df = train_test_split(\n",
    "        df_data, df_target, test_size=test_ratio, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Convert target variable to binary (assuming it's categorical with \">50K\" and others)\n",
    "    train_y_df = np.where(train_y_df == \">50K\", 1, 0)\n",
    "    test_y_df = np.where(test_y_df == \">50K\", 1, 0)\n",
    "\n",
    "    return train_X_df, train_y_df, test_X_df, test_y_df\n",
    "\n",
    "# Split your data into train and test splits\n",
    "\n",
    "train_X, train_y, test_X, test_y = train_test_split_df(df)\n",
    "\n",
    "print(len(train_X))\n",
    "print(len(train_y))\n",
    "print(len(test_X))\n",
    "print(len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a0c2dd",
   "metadata": {
    "id": "d6a0c2dd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(train_X.isnull().sum())\n",
    "print(test_X.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OcyxC1mKMPqK",
   "metadata": {
    "id": "OcyxC1mKMPqK"
   },
   "source": [
    "Step 1: encode features without missing values\n",
    "<br> `native-country`, `occupation` and `workclass` have missing values, so we first need to impute them before encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_OjRTt683uBW",
   "metadata": {
    "id": "_OjRTt683uBW"
   },
   "outputs": [],
   "source": [
    "# Ordinal Encoding for 'education'\n",
    "education_order = ['Preschool', '1st-4th', '5th-6th', '7th-8th', '9th', '10th',\n",
    "                   '11th', '12th', 'HS-grad', 'Some-college', 'Assoc-voc',\n",
    "                   'Assoc-acdm', 'Prof-school', 'Bachelors', 'Masters', 'Doctorate']\n",
    "ordinal_encoder = OrdinalEncoder(categories=[education_order])\n",
    "\n",
    "train_X['education_encoded'] = ordinal_encoder.fit_transform(train_X[['education']])\n",
    "test_X['education_encoded'] = ordinal_encoder.transform(test_X[['education']])\n",
    "\n",
    "# OneHotEncoding for nominal features without missing values and without an implied order\n",
    "nominal_columns_without_missing = ['marital-status', 'relationship', 'race', 'sex']  # These have no missing values\n",
    "onehot_encoder = #None\n",
    "\n",
    "# Fit on training data and apply to both train and test\n",
    "train_onehot_encoded = #None\n",
    "test_onehot_encoded = #None\n",
    "\n",
    "# Create DataFrames for one-hot encoded columns\n",
    "train_onehot_df = pd.DataFrame(train_onehot_encoded, columns=onehot_encoder.get_feature_names_out(nominal_columns_without_missing))\n",
    "test_onehot_df = pd.DataFrame(test_onehot_encoded, columns=onehot_encoder.get_feature_names_out(nominal_columns_without_missing))\n",
    "\n",
    "# Reset indices for consistency and combine data\n",
    "train_X = train_X.reset_index(drop=True)\n",
    "test_X = test_X.reset_index(drop=True)\n",
    "train_X = pd.concat([train_X, train_onehot_df], axis=1)\n",
    "test_X = pd.concat([test_X, test_onehot_df], axis=1)\n",
    "\n",
    "# Drop the original nominal and 'education' columns from both datasets\n",
    "train_X.drop(columns=nominal_columns_without_missing + ['education'], inplace=True)\n",
    "test_X.drop(columns=nominal_columns_without_missing + ['education'], inplace=True)\n",
    "\n",
    "# Verify the transformed datasets\n",
    "print(\"Training data (X):\")\n",
    "train_X.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ka-KB6-yLDwR",
   "metadata": {
    "id": "Ka-KB6-yLDwR"
   },
   "source": [
    "**Dataset A:** drop missing values (same as dataset in Task 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uBqgpIy_MZA9",
   "metadata": {
    "id": "uBqgpIy_MZA9"
   },
   "outputs": [],
   "source": [
    "# A: Dataset with dropped missing values\n",
    "# Drop rows with missing values in the training and testing datasets\n",
    "train_X_dropna = train_X.copy()\n",
    "test_X_dropna = test_X.copy()\n",
    "\n",
    "train_X_dropna = #None\n",
    "test_X_dropna = #None\n",
    "\n",
    "# Ensure alignment by dropping the corresponding rows in y\n",
    "train_y_dropna = #None\n",
    "test_y_dropna = #None\n",
    "\n",
    "# Display missing value counts for train and test datasets\n",
    "print(\"Missing values in training data after dropping rows:\")\n",
    "print(train_X_dropna.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in testing data after dropping rows:\")\n",
    "print(test_X_dropna.isnull().sum())\n",
    "\n",
    "# Display dataset lengths after dropping rows with missing values\n",
    "print(f\"\\n\\nDataset length after dropping rows containing NA (train): {len(train_X_dropna)}\")\n",
    "print(f\"Dataset length after dropping rows containing NA (test): {len(test_X_dropna)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dVqIV3pLoFy",
   "metadata": {
    "id": "3dVqIV3pLoFy"
   },
   "source": [
    "**Dataset B:** KNN imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "THNGlHYsNhKP",
   "metadata": {
    "id": "THNGlHYsNhKP"
   },
   "source": [
    "\n",
    "\n",
    "> NOTE: Typically you would not use KNN imputation for categorical columns.\n",
    "Instead use a more appropriate imputation method such as most frequent imputation\n",
    "since KNN is best suited to numerical variables.\n",
    "However to illustrate KNN imputation on this dataset here is a workaround\n",
    "by first ordinal encoding, then applying KNN, then converting back to original categories\n",
    "(this is not an ideal solution as ordinal encoding will introduce bias if the\n",
    "variable is not ordinal. Also, one-hot encoding and then KNN-imputation would not work, can you think why?).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "B2Crh-TB5MJG",
   "metadata": {
    "id": "B2Crh-TB5MJG"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Columns with missing values\n",
    "columns_with_missing_values = #None\n",
    "\n",
    "# Apply KNN Imputation separately for train and test\n",
    "# Training data\n",
    "train_X_knn_imputed = train_X.copy()\n",
    "\n",
    "# Temporarily encode categorical columns with missing values\n",
    "temp_encoder = OrdinalEncoder()\n",
    "train_temp = train_X[columns_with_missing_values].copy()\n",
    "train_temp_encoded = temp_encoder.fit_transform(train_temp)\n",
    "\n",
    "# Fit KNN imputer on the training data\n",
    "knn_imputer = #None\n",
    "train_imputed_data = #None\n",
    "\n",
    "# Decode the categorical columns back to original categories for training data\n",
    "train_imputed_data_decoded = temp_encoder.inverse_transform(train_imputed_data)\n",
    "train_imputed_final = pd.DataFrame(train_imputed_data_decoded, columns=columns_with_missing_values)\n",
    "\n",
    "# Integrate the imputed columns back into the training DataFrame\n",
    "train_X_knn_imputed[columns_with_missing_values] = train_imputed_final\n",
    "\n",
    "# Testing data\n",
    "test_X_knn_imputed = test_X.copy()\n",
    "\n",
    "# Temporarily encode categorical columns with missing values\n",
    "test_temp = test_X[columns_with_missing_values].copy()\n",
    "test_temp_encoded = temp_encoder.transform(test_temp)  # Use the encoder fitted on training data\n",
    "\n",
    "# Apply KNN imputer to the testing data (fit is not called again)\n",
    "test_imputed_data = #None\n",
    "\n",
    "# Decode the categorical columns back to original categories for testing data\n",
    "test_imputed_data_decoded = temp_encoder.inverse_transform(test_imputed_data)\n",
    "test_imputed_final = pd.DataFrame(test_imputed_data_decoded, columns=columns_with_missing_values)\n",
    "\n",
    "# Integrate the imputed columns back into the testing DataFrame\n",
    "test_X_knn_imputed[columns_with_missing_values] = test_imputed_final\n",
    "\n",
    "# Verify missing values in the resulting datasets\n",
    "print(\"Missing values in training data after KNN imputation:\")\n",
    "print(train_X_knn_imputed.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in testing data after KNN imputation:\")\n",
    "print(test_X_knn_imputed.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UeigNdLbOQtQ",
   "metadata": {
    "id": "UeigNdLbOQtQ"
   },
   "outputs": [],
   "source": [
    "# check resulting df\n",
    "train_X_knn_imputed.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OEJ20ds-MjON",
   "metadata": {
    "id": "OEJ20ds-MjON"
   },
   "source": [
    "**Dataset C:** Most frequent imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nDKc5BURMkHl",
   "metadata": {
    "id": "nDKc5BURMkHl"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Columns with missing values\n",
    "columns_with_missing_values = #None\n",
    "\n",
    "# Training data\n",
    "train_X_mode_imputed = train_X.copy()\n",
    "\n",
    "# Create an imputer object using the most frequent strategy and fit on training data\n",
    "mode_imputer = #None\n",
    "train_X_mode_imputed[columns_with_missing_values] = mode_imputer.#None\n",
    "\n",
    "# Testing data\n",
    "test_X_mode_imputed = test_X.copy()\n",
    "\n",
    "# Apply the trained imputer to the testing data\n",
    "test_X_mode_imputed[columns_with_missing_values] = mode_imputer.transform(test_X_mode_imputed[columns_with_missing_values])\n",
    "\n",
    "# Verify missing values in the resulting datasets\n",
    "print(\"Missing values in training data after mode imputation:\")\n",
    "print(train_X_mode_imputed.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in testing data after mode imputation:\")\n",
    "print(test_X_mode_imputed.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RhQ7OIhmPPkK",
   "metadata": {
    "id": "RhQ7OIhmPPkK"
   },
   "source": [
    "Now that we have done data imputation, we can **encode the columns that had missing values** (`native-country`, `occupation` and `workclass`).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QMXpDPelPk12",
   "metadata": {
    "id": "QMXpDPelPk12"
   },
   "source": [
    "\n",
    "\n",
    "> What type(s) of encoding is appropriate here?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zIi-nJ2eA8mD",
   "metadata": {
    "id": "zIi-nJ2eA8mD"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Function to perform one-hot encoding\n",
    "def apply_onehot_encoding(train_df, test_df, columns, encoder=None):\n",
    "    # Fit OneHotEncoder on training data if no encoder is passed\n",
    "    if encoder is None:\n",
    "        encoder = #None\n",
    "        encoder.fit(#None)\n",
    "\n",
    "    # Transform both training and testing datasets\n",
    "    train_encoded = encoder.#None\n",
    "    test_encoded = encoder.#None\n",
    "\n",
    "    # Create DataFrames for the encoded columns\n",
    "    column_names = encoder.get_feature_names_out(columns)\n",
    "    train_encoded_df = pd.DataFrame(train_encoded, columns=column_names)\n",
    "    test_encoded_df = pd.DataFrame(test_encoded, columns=column_names)\n",
    "\n",
    "    # Reset indices to ensure alignment and concatenate\n",
    "    train_df_reset = train_df.reset_index(drop=True)\n",
    "    test_df_reset = test_df.reset_index(drop=True)\n",
    "\n",
    "    train_df_final = pd.concat([train_df_reset.drop(columns, axis=1), train_encoded_df.reset_index(drop=True)], axis=1)\n",
    "    test_df_final = pd.concat([test_df_reset.drop(columns, axis=1), test_encoded_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    return train_df_final, test_df_final, encoder\n",
    "\n",
    "\n",
    "# Columns to encode\n",
    "columns_to_encode = #None\n",
    "\n",
    "# Apply one-hot encoding for each imputed dataset (dropna, KNN, mode)\n",
    "\n",
    "# 1. Dropna\n",
    "#None\n",
    "\n",
    "\n",
    "# 2. KNN Imputed\n",
    "#None\n",
    "\n",
    "\n",
    "# 3. Mode Imputed\n",
    "#None\n",
    "\n",
    "# Verify if any missing values remain after encoding\n",
    "print(\"Missing values in dropna-encoded training data:\", train_X_dropna_encoded.isnull().sum().sum())\n",
    "print(\"Missing values in dropna-encoded testing data:\", test_X_dropna_encoded.isnull().sum().sum())\n",
    "\n",
    "print(\"\\nMissing values in KNN-encoded training data:\", train_X_knn_encoded.isnull().sum().sum())\n",
    "print(\"Missing values in KNN-encoded testing data:\", test_X_knn_encoded.isnull().sum().sum())\n",
    "\n",
    "print(\"\\nMissing values in mode-encoded training data:\", train_X_mode_encoded.isnull().sum().sum())\n",
    "print(\"Missing values in mode-encoded testing data:\", test_X_mode_encoded.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a28f3f",
   "metadata": {
    "id": "72a28f3f"
   },
   "source": [
    "### 3.3 Train a classifier to predict outcome\n",
    "\n",
    "Example: SVM or KNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UH7K_cuLP8HM",
   "metadata": {
    "id": "UH7K_cuLP8HM"
   },
   "source": [
    "**Task**: Train a classifier on the different datasets to compare imputation method accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9g0siN9fZKdj",
   "metadata": {
    "id": "9g0siN9fZKdj"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Non-categorical features to scale\n",
    "non_categorical_features = #None\n",
    "\n",
    "def scale_features(train_X, test_X, features_to_scale):\n",
    "    scaler =#None\n",
    "\n",
    "    # Fit scaler on training data and transform both train and test sets\n",
    "    train_scaled = #None\n",
    "    test_scaled = #None\n",
    "\n",
    "    # Replace scaled features in the original DataFrame, maintaining indices\n",
    "    train_X_scaled = train_X.copy()\n",
    "    test_X_scaled = test_X.copy()\n",
    "    train_X_scaled[features_to_scale] = pd.DataFrame(train_scaled, index=train_X.index, columns=features_to_scale)\n",
    "    test_X_scaled[features_to_scale] = pd.DataFrame(test_scaled, index=test_X.index, columns=features_to_scale)\n",
    "\n",
    "    return train_X_scaled, test_X_scaled\n",
    "\n",
    "\n",
    "# Define classifiers\n",
    "classifiers = {\n",
    "    #None\n",
    "}\n",
    "\n",
    "# Datasets for each imputation type\n",
    "datasets = {\n",
    "    #None\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Train and test classifiers on each dataset\n",
    "for imputation_type, (train_X, train_y, test_X, test_y) in datasets.items():\n",
    "    # Apply scaling to non-categorical features\n",
    "    #None\n",
    "\n",
    "    for classifier_name, classifier in classifiers.items():\n",
    "        # Train the classifier\n",
    "        #None\n",
    "\n",
    "        # Predict on the test set\n",
    "        #None\n",
    "\n",
    "        # Calculate accuracy\n",
    "        #None\n",
    "\n",
    "        # Append the result\n",
    "        results.append({\n",
    "            'Imputation Type': imputation_type,\n",
    "            'Classifier': classifier_name,\n",
    "            'Accuracy': accuracy\n",
    "        })\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Pivot the DataFrame to organize results by Imputation Type and Classifier\n",
    "pivot_df = results_df.pivot(index='Imputation Type', columns='Classifier', values='Accuracy')\n",
    "\n",
    "# Display the table\n",
    "print(\"\\nClassifier Performance Across Imputation Types with Scaling:\")\n",
    "print(pivot_df.to_string(float_format=\"%.2f\"))  # Display with two decimal places\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
